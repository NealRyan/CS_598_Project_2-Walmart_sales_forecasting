{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import patsy\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(4031)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file locations and names\n",
    "\n",
    "project_root_dir = \"Data\"\n",
    "project_subdir_prefix = \"fold_\"\n",
    "train_data_filename = \"train.csv\"\n",
    "test_data_filename = \"test.csv\"\n",
    "\n",
    "\n",
    "# The number of train/test data folders and the target RMSE for each\n",
    "# train/test split in each folder\n",
    "\n",
    "n_datasets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of data subfolders, each with a separate training and test set.\n",
    "# fold1 - fold5 have target RMSE 0.125, and fold6 - fold10 have target RMSE 0.135.\n",
    "\n",
    "os_walk = os.walk(project_root_dir)\n",
    "data_subdir_list = [subdirs for root, subdirs, files in os_walk][0]\n",
    "n_subdirs = len(data_subdir_list)\n",
    "\n",
    "assert(n_subdirs == n_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for training and test datasets\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "\n",
    "# Loop over subfolders and read in training/test datasets and test house sale prices.\n",
    "# Use a loop instead of using os.walk directly to avoid \"fold10\" immediately following \"fold1\".\n",
    "\n",
    "for subdir_num in np.arange(n_subdirs) + 1:\n",
    "    subdir_num_str = str(subdir_num)\n",
    "    train_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   train_data_filename)))\n",
    "    test_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   test_data_filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a WMAE function for scoring\n",
    "\n",
    "def wmae():\n",
    "    file_path = 'Data/test_with_label.csv'\n",
    "    test = pd.read_csv(file_path)\n",
    "    num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    return wae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through data and pull apart date into week and year\n",
    "def preprocess(data):\n",
    "    #Split date into useful features\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "\n",
    "    #One hot encode Wk\n",
    "    data = pd.get_dummies(data, columns=['Wk'], prefix='Week ')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(n_datasets):\n",
    "    train = train_datasets[j]\n",
    "    test = test_datasets[j]\n",
    "\n",
    "    # Initialize the DataFrame to store predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    fold_train = preprocess(train)\n",
    "    fold_test = preprocess(test)\n",
    "\n",
    "    stores = fold_train[\"Store\"].unique()\n",
    "    depts = fold_train[\"Dept\"].unique()\n",
    "\n",
    "    for store in stores:\n",
    "        for dept in depts:\n",
    "            #Find training and test data within same store and then same department\n",
    "            train = fold_train[(fold_train[\"Store\"] == store) & (fold_train[\"Dept\"] == dept)]\n",
    "            test = fold_test[(fold_test[\"Store\"] == store) & (fold_test[\"Dept\"] == dept)]\n",
    "\n",
    "            #abort if the train data is non-existant (i.e., this combo of store/dept doesnt appear in data)\n",
    "            if len(train) == 0:\n",
    "                continue\n",
    "\n",
    "            Y_train = train[\"Weekly_Sales\"]\n",
    "            X_train = train.drop([\"Weekly_Sales\", \"Date\"], axis=1)\n",
    "\n",
    "            X_test = test\n",
    "\n",
    "            #Keep Store, dept, and date info for later merging\n",
    "            tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "            X_test = X_test.drop([\"Date\"], axis=1)\n",
    "\n",
    "\n",
    "            #Implement SVD\n",
    "            #n_components=5\n",
    "            #svd_df = pd.DataFrame(svd_result, columns=[[f'SVD_{i}' for i in range (n_components)]]) \n",
    "            svd = TruncatedSVD() \n",
    "            svd_result = svd.fit_transform(X_train)\n",
    "            svd_df = pd.DataFrame(svd_result) \n",
    "\n",
    "            #Train model on only the features SVD selected\n",
    "            model = sm.OLS(Y_train, svd_df).fit()\n",
    "            mycoef = model.params.fillna(0)\n",
    "            \n",
    "            #Fit SVD columns for test set\n",
    "            X_test = svd.transform(X_test)\n",
    "            \n",
    "            #Predict Y\n",
    "            tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "\n",
    "            #Readd context of store, dept, and date\n",
    "            test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "            \n",
    "        \n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "    # Save the output to CSV\n",
    "    file_path = f'Data/fold_{j+1}/mypred.csv'\n",
    "    print(f'fold_{j+1} processed')\n",
    "    test_pred.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Code (OLS only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS method requires a different preprocess function\n",
    "\n",
    "def preprocess(data):\n",
    "    #Split date into useful features\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_1 processed\n",
      "fold_2 processed\n",
      "fold_3 processed\n",
      "fold_4 processed\n",
      "fold_5 processed\n",
      "fold_6 processed\n",
      "fold_7 processed\n",
      "fold_8 processed\n",
      "fold_9 processed\n",
      "fold_10 processed\n"
     ]
    }
   ],
   "source": [
    "#Original OLS model\n",
    "\n",
    "for j in range(n_datasets):\n",
    "    train = train_datasets[j]\n",
    "    test = test_datasets[j]\n",
    "\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "    train_split = preprocess(train_split)\n",
    "    y, X = patsy.dmatrices('Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr  + Wk', \n",
    "                        data = train_split, \n",
    "                        return_type='dataframe')\n",
    "    train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices('Yr ~ Store + Dept + Yr  + Wk', \n",
    "                        data = test_split, \n",
    "                        return_type='dataframe')\n",
    "    X['Date'] = test_split['Date']\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "\n",
    "    for key in keys:\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "    \n",
    "        Y = X_train['Weekly_Sales']\n",
    "        X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)\n",
    "        \n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        \n",
    "    \n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "\n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-10:\n",
    "                    cols_to_drop.append(col_name)\n",
    "                \n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "        \n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "        \n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "        \n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "    # Save the output to CSV\n",
    "    file_path = f'Data/fold_{j+1}/mypred.csv'\n",
    "    print(f'fold_{j+1} processed')\n",
    "    test_pred.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t7170.060\n",
      "\t7392.972\n",
      "\t7604.202\n",
      "\t6603.801\n",
      "\t7567.474\n",
      "\t9535.512\n",
      "\t7857.230\n",
      "\t6024.099\n",
      "\t6104.525\n",
      "\t5025.340\n",
      "7088.522\n"
     ]
    }
   ],
   "source": [
    "wae = wmae()\n",
    "for value in wae:\n",
    "    print(f\"\\t{value:.3f}\")\n",
    "print(f\"{sum(wae) / len(wae):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear errors\n",
    "\n",
    "\t2049.347\n",
    "\t1467.113\n",
    "\t1446.882\n",
    "\t1595.628\n",
    "\t2334.678\n",
    "\t1675.221\n",
    "\t1720.828\n",
    "\t1427.286\n",
    "\t1443.787\n",
    "\t1444.677\n",
    "1660.545"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
