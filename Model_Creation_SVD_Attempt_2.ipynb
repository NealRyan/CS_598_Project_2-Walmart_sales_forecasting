{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "\n",
    "# Import Pandas, ignoring SettingWithCopyWarning notifications\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "\n",
    "# Set seed\n",
    "SEED = 4031\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file locations and names\n",
    "\n",
    "project_root_dir = \"Data\"\n",
    "project_subdir_prefix = \"fold_\"\n",
    "train_data_filename = \"train.csv\"\n",
    "test_data_filename = \"test.csv\"\n",
    "\n",
    "\n",
    "# The number of train/test data folders and the target RMSE for each\n",
    "# train/test split in each folder\n",
    "\n",
    "n_datasets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of data subfolders, each with a separate training and test set.\n",
    "\n",
    "os_walk = os.walk(project_root_dir)\n",
    "data_subdir_list = [subdirs for root, subdirs, files in os_walk][0]\n",
    "n_subdirs = len(data_subdir_list)\n",
    "\n",
    "assert(n_subdirs == n_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for training and test datasets\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "\n",
    "# Loop over subfolders and read in training/test datasets and test weekly sales.\n",
    "# Use a loop instead of using os.walk directly to avoid \"fold10\" immediately following \"fold1\".\n",
    "\n",
    "for subdir_num in np.arange(n_subdirs) + 1:\n",
    "    subdir_num_str = str(subdir_num)\n",
    "    train_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   train_data_filename)))\n",
    "    test_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   test_data_filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a WMAE function for scoring\n",
    "\n",
    "def wmae():\n",
    "    file_path = 'Data/test_with_label.csv'\n",
    "    test = pd.read_csv(file_path)\n",
    "    num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    return wae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit original OLS: group stores by department and add SVD/PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps\n",
    "1. Group training data by department\n",
    "2. Pivot each department's data so that stores are rows and dates are columns, with values = weekly sales\n",
    "3. Fill in missing stores and dates, setting their sales to zero\n",
    "4. Center store values\n",
    "5. Perform SVD\n",
    "6. Re-add store means\n",
    "7. Use the SVD output as y_train for x_train = \\[Year, Week, Store\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components to return from SVD. This is from the example in Campuswire post #364:\n",
    "# https://campuswire.com/c/G06C55090/feed/364\n",
    "n_components = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fill_and_pivot(train_data):\n",
    "    \"\"\"\n",
    "    Prep training data for smoothing of the weekly sales figures.\n",
    "    Given a training set of stores, departments, dates and sales figures,\n",
    "    fill in any stores and dates missing for any given department.\n",
    "    Fill in zeroes for the missing sales figures.\n",
    "    This gives each department's data an identical shape.\n",
    "    Finally, return a pivoted dataset with weeks as columns and sales figures as values.\n",
    "    Also return dictionaries of stores and dates in the training set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get unique stores and dates. This is to help fill in zeroes for any departments\n",
    "    # that are missing from certain stores and dates.\n",
    "    train_store_list = train_data[\"Store\"].unique().tolist()\n",
    "    train_date_list = train_data[\"Date\"].unique().tolist()\n",
    "    \n",
    "    # Get columns needed for further processing\n",
    "    train_sel_col = train_data[[\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\"]]\n",
    "    \n",
    "    # Fill in missing stores and sales dates dates for any given department.\n",
    "    \n",
    "    # Set Date to categorical to help fill in sales dates missing for any stores/depts\n",
    "    train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n",
    "    train_sel_col = train_sel_col.groupby([\"Store\", \"Dept\", \"Date\"], as_index=False).first()\n",
    "    # Set Date column back to datetime from categorical\n",
    "    train_sel_col.loc[:, [\"Date\"]] = pd.to_datetime(train_sel_col[\"Date\"].values)\n",
    "    \n",
    "    # Set Store to categorical. This helps fill in stores missing for any dates/depts.\n",
    "    train_sel_col.loc[:, [\"Store\"]] = pd.Categorical(train_sel_col[\"Store\"].values, categories=train_store_list)\n",
    "    train_sel_col = train_sel_col.groupby([\"Store\", \"Dept\", \"Date\"], as_index=False).first()\n",
    "    # Set store back to numeric\n",
    "    train_sel_col.loc[:, [\"Store\"]] = train_sel_col[\"Store\"].values.astype(int)\n",
    "    \n",
    "    # Pivot dataframe so that stores are rows and sales dates columns, with values = Weekly_Sales.\n",
    "    # Fill in missing values with zeroes.\n",
    "    train_pivot = train_sel_col.pivot(index=[\"Dept\", \"Store\"], columns=\"Date\", values=\"Weekly_Sales\").reset_index().fillna(0)\n",
    "\n",
    "    return train_pivot, train_store_list, train_date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_weekly_sales(train_data, train_store_list, train_date_list, n_components=n_components):\n",
    "    \"\"\"\n",
    "    Given a department's pivoted training dataset of weekly sales,\n",
    "    use SVD to smooth the weekly sales across stores.\n",
    "    Return the pivoted dataset with smoothed sales.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract numpy array of sales figures\n",
    "    train_sales = train_data.to_numpy()[:, 2:]\n",
    "    \n",
    "    # Get store means\n",
    "    train_store_means = np.mean(train_sales, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Center sales data\n",
    "    train_centered_sales = train_sales - train_store_means\n",
    "    \n",
    "    # Perform SVD on centered sales data\n",
    "    train_U, train_S, train_V = np.linalg.svd(train_centered_sales)\n",
    "    \n",
    "    # Reduce the number of components\n",
    "    train_U_reduced = train_U[:, :n_components]\n",
    "    train_D_reduced = np.diag(train_S[:n_components])\n",
    "    train_Vt_reduced = train_V[:, :n_components].T\n",
    "    \n",
    "    # Regenerate smoothed sales\n",
    "    train_sales_smooth = (train_U_reduced @ train_D_reduced @ train_Vt_reduced) + train_store_means\n",
    "    \n",
    "    # Convert smoothed sales from array to dataframe\n",
    "    train_sales_smooth_df = pd.DataFrame(train_sales_smooth, columns=train_date_list)\n",
    "    # Re-add Store column\n",
    "    train_sales_smooth_df[\"Store\"] = train_store_list\n",
    "\n",
    "    # Return results\n",
    "    return train_sales_smooth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unpivot(train_pivot):\n",
    "    \"\"\"\n",
    "    Unpivot a department's smoothed weekly sales figures back to a form\n",
    "    of one figure per store and date.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpivot the dataframe\n",
    "    train_unpivot = train_pivot.melt(id_vars=[\"Store\"], var_name=\"Date\", value_name=\"Weekly_Sales\")\n",
    "    \n",
    "    return train_unpivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_to_years_and_weeks(data):\n",
    "    \"\"\"\n",
    "    Convert sales dates in data to numeric years and categorical weeks.\n",
    "    Separate the date from the years and weeks.\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = pd.to_datetime(data[\"Date\"])\n",
    "    data[\"Wk\"] = tmp.dt.isocalendar().week\n",
    "    data[\"Yr\"] = tmp.dt.year\n",
    "    data[\"Wk\"] = pd.Categorical(data[\"Wk\"], categories=[i for i in range(1, 53)])  # 52 weeks\n",
    "    \n",
    "    date_vals = data[\"Date\"]\n",
    "    \n",
    "    # Drop date column\n",
    "    data.drop([\"Date\"], axis=1, inplace=True)\n",
    "    \n",
    "    return data, date_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_1 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_2 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_3 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_4 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_5 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_6 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_7 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_8 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_9 processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuohy\\AppData\\Local\\Temp\\ipykernel_41404\\1763239482.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_sel_col.loc[:, [\"Date\"]] = pd.Categorical(train_sel_col[\"Date\"].values, categories=train_date_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_10 processed\n"
     ]
    }
   ],
   "source": [
    "# Loop over folds\n",
    "for j in range(n_datasets):\n",
    "\n",
    "    # Get a pair of training and test sets\n",
    "    train = train_datasets[j]\n",
    "    test = test_datasets[j]\n",
    "\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    # Identify the distinct store/dept pairs shared by the training and test set.\n",
    "    # Will only process these.\n",
    "\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "    \n",
    "    # Join the distinct store/dept pairs to the training set.\n",
    "    # Why left join? When would training data not be available?\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "    \n",
    "    # Now join the distinct store/dept pairs to the test set.\n",
    "    # Same question: why left join? When would training data not be available?\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "\n",
    "    # Pivot training sales by week, and fill in missing stores and dates for each department.\n",
    "    # Get lists of stores and dates present in training.\n",
    "    train_pivot, train_store_list, train_date_list = train_fill_and_pivot(train_split)\n",
    "        \n",
    "    # Group by department to help build separate model for each department.\n",
    "    # Create dict where key = Dept and value = dataframe of Store/Date/Weekly_Sales.\n",
    "    #train_split = dict(tuple(train_split.groupby([\"Dept\"])))\n",
    "    train_split = dict(tuple(train_pivot.groupby([\"Dept\"])))\n",
    "    test_split = dict(tuple(test_split.groupby([\"Dept\"])))\n",
    "    \n",
    "    # Get the training departments\n",
    "    depts = list(train_split)\n",
    "\n",
    "    # Loop over (store, dept) tuples\n",
    "    for dept in depts:\n",
    "        \n",
    "        #print(\"Dept:\", dept)\n",
    "\n",
    "        # Get training and test design matrices corresponding to (store, dept)\n",
    "        X_train = train_split[dept]\n",
    "        #if (dept == 1):\n",
    "        #    print(\"X_train columns before smoothing\", X_train.columns)\n",
    "        X_test = test_split[dept]\n",
    "    \n",
    "        # Use SVD to smooth weekly sales in training data\n",
    "        X_train_smooth = smooth_weekly_sales(train_data=X_train,\n",
    "                                             train_store_list=train_store_list,\n",
    "                                             train_date_list=train_date_list,\n",
    "                                             n_components=n_components)\n",
    "        \n",
    "\n",
    "        # Unpivot the training data\n",
    "        X_train = train_unpivot(X_train_smooth)\n",
    "        \n",
    "        #if (dept == 1):\n",
    "        #    smoothed_train_file_path = f'Data/fold_{j+1}/smooth_train_dept_1.csv'\n",
    "        #    X_train.to_csv(smoothed_train_file_path, index=False)\n",
    "            \n",
    "        # Convert sales dates to years and weeks\n",
    "        X_train, train_dates = dates_to_years_and_weeks(X_train)\n",
    "        X_test, test_dates = dates_to_years_and_weeks(X_test)\n",
    "        \n",
    "        # Create final design matrix for training.\n",
    "        # This one-hot encodes Wk and creates an Intercept field.\n",
    "        \n",
    "        y_train, X_train = patsy.dmatrices(\"Weekly_Sales ~ Store + Yr + Wk\",\n",
    "                                          data = X_train,\n",
    "                                          return_type=\"dataframe\")\n",
    "        \n",
    "        # Create equivalent design matrix for test.\n",
    "        \n",
    "        y_tmp, X_test = patsy.dmatrices(\"Yr ~ Store + Yr + Wk\",\n",
    "                                          data = X_test,\n",
    "                                          return_type=\"dataframe\")\n",
    "        \n",
    "        # Split weekly sales from training predictors\n",
    "        #y_train = X_train[\"Weekly_Sales\"]\n",
    "        #X_train = X_train.drop([\"Weekly_Sales\"], axis=1)\n",
    "        \n",
    "        #print(\"y_train NANs:\", y_train.isna().sum())\n",
    "        \n",
    "        # Standardize column  order\n",
    "        #X_train = X_train[[\"Store\", \"Yr\", \"Wk\"]]\n",
    "        #X_test = X_test[[\"Store\", \"Yr\", \"Wk\"]]\n",
    "        \n",
    "        # Exclude store number from model fitting and prediction\n",
    "        X_train.drop([\"Store\"], axis=1, inplace=True)\n",
    "        test_store = X_test[\"Store\"]\n",
    "        X_test.drop([\"Store\"], axis=1, inplace=True)\n",
    "        \n",
    "        # Drop columns that are all zero in X_train\n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "        \n",
    "        # Identify and remove columns that are highly collinear in X_train\n",
    "        # with columns to their left.\n",
    "        # Note that this doesn't check the Intercept column.\n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "\n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-10:\n",
    "                    cols_to_drop.append(col_name)\n",
    "                \n",
    "        # Drop those collinear columns from both training and test X.\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "        \n",
    "        #if (dept == 1):\n",
    "        #    print(\"X_train columns after collinearity handling\", X_train.columns)\n",
    "        #    print(\"X_test columns after collinearity handling\", X_test.columns)\n",
    "\n",
    "        # Fit OLS model\n",
    "        model = sm.OLS(y_train, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "        \n",
    "        # Initialize dataframe to store predictors + predicted sales\n",
    "        tmp_pred = X_test\n",
    "        \n",
    "        # Exclude store number from predictions\n",
    "        \n",
    "        # Predict and save test sales\n",
    "        tmp_pred[\"Weekly_Pred\"] = np.dot(X_test, mycoef)\n",
    "\n",
    "        # Re-add Store, Department and Date fields\n",
    "        tmp_pred[\"Store\"] = test_store\n",
    "        tmp_pred[\"Dept\"] = dept\n",
    "        tmp_pred[\"Date\"] = test_dates\n",
    "\n",
    "        # Append this fold's predictions to the list\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "        \n",
    "    # Fill in any missing predictions with zero\n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "    # Save the output to CSV\n",
    "    file_path = f'Data/fold_{j+1}/mypred.csv'\n",
    "    print(f'fold_{j+1} processed')\n",
    "    test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t8990.059\n",
      "\t9187.492\n",
      "\t9193.391\n",
      "\t8862.238\n",
      "\t11264.484\n",
      "\t8541.695\n",
      "\t8563.644\n",
      "\t8821.667\n",
      "\t8667.612\n",
      "\t8477.072\n",
      "9056.936\n"
     ]
    }
   ],
   "source": [
    "wae = wmae()\n",
    "for value in wae:\n",
    "    print(f\"\\t{value:.3f}\")\n",
    "print(f\"{sum(wae) / len(wae):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
