{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "\n",
    "# Set seed\n",
    "SEED = 4031\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file locations and names\n",
    "\n",
    "project_root_dir = \"Data\"\n",
    "project_subdir_prefix = \"fold_\"\n",
    "train_data_filename = \"train.csv\"\n",
    "test_data_filename = \"test.csv\"\n",
    "\n",
    "\n",
    "# The number of train/test data folders and the target RMSE for each\n",
    "# train/test split in each folder\n",
    "\n",
    "n_datasets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of data subfolders, each with a separate training and test set.\n",
    "\n",
    "os_walk = os.walk(project_root_dir)\n",
    "data_subdir_list = [subdirs for root, subdirs, files in os_walk][0]\n",
    "n_subdirs = len(data_subdir_list)\n",
    "\n",
    "assert(n_subdirs == n_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for training and test datasets\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "\n",
    "# Loop over subfolders and read in training/test datasets and test weekly sales.\n",
    "# Use a loop instead of using os.walk directly to avoid \"fold10\" immediately following \"fold1\".\n",
    "\n",
    "for subdir_num in np.arange(n_subdirs) + 1:\n",
    "    subdir_num_str = str(subdir_num)\n",
    "    train_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   train_data_filename)))\n",
    "    test_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   test_data_filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myeval():\n",
    "    file_path = 'Data/test_with_label.csv'\n",
    "    test_with_label = pd.read_csv(file_path)\n",
    "    num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'Data/fold_{i+1}/test.csv'\n",
    "        test = pd.read_csv(file_path)\n",
    "        test = test.drop(columns=['IsHoliday']).merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "\n",
    "        file_path = f'Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    return wae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean negative sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for dataset in train_datasets:\n",
    "#    dataset.loc[dataset['Weekly_Sales'] < 0, 'Weekly_Sales'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit original OLS: group stores by department and add SVD/PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps\n",
    "1. Group training data by department\n",
    "2. Pivot each department's data so that stores are rows and dates are columns, with values = weekly sales\n",
    "3. Fill in missing stores and dates, setting their sales to zero\n",
    "4. Center store values\n",
    "5. Perform SVD\n",
    "6. Re-add store means\n",
    "7. Use the SVD output as y_train for x_train = \\[Year, Week, Store\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components to return from SVD. This is from the example in Campuswire post #364:\n",
    "# https://campuswire.com/c/G06C55090/feed/364\n",
    "n_components = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_weekly_sales(train_data, fold, n_components=n_components):\n",
    "    \"\"\"\n",
    "    Given a fold's training dataset of weekly sales by store and department,\n",
    "    use SVD to smooth each department's weekly sales across stores.\n",
    "    Return the dataset with smoothed sales.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store each department's processed data\n",
    "    #dept_store_list = []\n",
    "    smoothed_sales_list = []\n",
    "    \n",
    "    t_split = dict(tuple(train_data.groupby([\"Dept\"])))\n",
    "    \n",
    "    depts = list(t_split)\n",
    "    \n",
    "    for dept in depts:\n",
    "        \n",
    "        # Get needed columns for department\n",
    "        t_data = t_split[dept]\n",
    "        \n",
    "        # Pivot each store/dept combo's sales by date.\n",
    "        # Rows and stores are depts, columns are dates, values are sales figures.\n",
    "        t_pivot = t_data.pivot(index=[\"Store\", \"Dept\"], columns=\"Date\", values=\"Weekly_Sales\").reset_index().fillna(0)\n",
    "\n",
    "\n",
    "        # Save list of Date values for dept\n",
    "        t_dates = t_pivot.columns[2:]\n",
    "\n",
    "      \n",
    "        # Extract just the dates and sales for further processing\n",
    "        t_sales = t_pivot.to_numpy()[:, 2:]\n",
    "        t_dept_store = t_pivot.to_numpy()[:, :2]\n",
    "        \n",
    "        # Get store means\n",
    "        t_store_means = np.mean(t_sales, axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "        # Center the sales by store\n",
    "        t_centered_sales = t_sales - t_store_means\n",
    "        \n",
    "        # Perform SVD on centered sales data if there are more than n_components components\n",
    "        if t_centered_sales.shape[0] > n_components:\n",
    "            U, S, V = np.linalg.svd(t_centered_sales)\n",
    "            \n",
    "            # Keep only the top n_components components\n",
    "            U_reduced = U[:, :n_components]\n",
    "            S_reduced = np.diag(S[:n_components])\n",
    "            V_reduced = V[:n_components, :]\n",
    "            \n",
    "            # Reconstruct smoothed sales\n",
    "            t_sales_smooth = np.dot(U_reduced, np.dot(S_reduced, V_reduced)) + t_store_means\n",
    "        else:\n",
    "            # If there are not enough components to perform SVD, use original sales\n",
    "            t_sales_smooth = t_sales\n",
    "        \n",
    "        #print(t_sales_smooth)\n",
    "        # Join Store and Dept back to smoothed sales figures\n",
    "        t_dept_store_sales_smooth = np.concatenate((t_dept_store, t_sales_smooth), axis=1)\n",
    "        t_columns = [\"Store\", \"Dept\"] + t_dates.tolist()\n",
    "        \n",
    "        #print(\"t_columns:\", t_columns)\n",
    "    \n",
    "        # Convert smoothed sales from array to dataframe\n",
    "        t_dept_store_sales_smooth_df = pd.DataFrame(t_dept_store_sales_smooth, columns=t_columns)\n",
    "\n",
    "        # Unpivot the smoothed sales\n",
    "        t_sales_unpivot = t_dept_store_sales_smooth_df.melt(id_vars=[\"Store\", \"Dept\"], \n",
    "                                                            var_name=\"Date\",\n",
    "                                                            value_name=\"Weekly_Sales\").reset_index(drop=True)\n",
    "\n",
    "        # Save dept's smoothed sales\n",
    "        smoothed_sales_list.append(t_sales_unpivot)\n",
    "\n",
    "\n",
    "    \n",
    "    # Stack the accumulated smoothed sales\n",
    "    t_dept_store_sales = pd.concat(smoothed_sales_list, axis=0, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    return t_dept_store_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_to_years_and_weeks(data):\n",
    "    \"\"\"\n",
    "    Convert sales dates in data to numeric years and categorical weeks.\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = pd.to_datetime(data[\"Date\"])\n",
    "    data[\"Wk\"] = tmp.dt.isocalendar().week\n",
    "    data[\"Yr\"] = tmp.dt.year\n",
    "    data[\"Wk\"] = pd.Categorical(data[\"Wk\"], categories=[i for i in range(1, 53)])  # 52 weeks\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\spect\\OneDrive\\Documents\\GitHub\\CS_598_Project_2-Walmart_sales_forecasting\\Model_Creation_and_SVD.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spect/OneDrive/Documents/GitHub/CS_598_Project_2-Walmart_sales_forecasting/Model_Creation_and_SVD.ipynb#X20sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m X[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m test_split[\u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spect/OneDrive/Documents/GitHub/CS_598_Project_2-Walmart_sales_forecasting/Model_Creation_and_SVD.ipynb#X20sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# Get dictionary where keys are (Store, Dept) tuples, and values are the\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spect/OneDrive/Documents/GitHub/CS_598_Project_2-Walmart_sales_forecasting/Model_Creation_and_SVD.ipynb#X20sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# \\\"Yr  + Wk + Date\\\" design matrices corresponding to each key.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/spect/OneDrive/Documents/GitHub/CS_598_Project_2-Walmart_sales_forecasting/Model_Creation_and_SVD.ipynb#X20sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m test_split \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mtuple\u001b[39;49m(X\u001b[39m.\u001b[39;49mgroupby([\u001b[39m'\u001b[39;49m\u001b[39mStore\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mDept\u001b[39;49m\u001b[39m'\u001b[39;49m])))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spect/OneDrive/Documents/GitHub/CS_598_Project_2-Walmart_sales_forecasting/Model_Creation_and_SVD.ipynb#X20sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Get the training departments\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/spect/OneDrive/Documents/GitHub/CS_598_Project_2-Walmart_sales_forecasting/Model_Creation_and_SVD.ipynb#X20sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(train_split)\n",
      "File \u001b[1;32mc:\\Users\\spect\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:763\u001b[0m, in \u001b[0;36mBaseGroupBy.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    762\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "File \u001b[1;32mc:\\Users\\spect\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:816\u001b[0m, in \u001b[0;36mBaseGroupBy.groups\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    771\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    772\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgroups\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[Hashable, np\u001b[39m.\u001b[39mndarray]:\n\u001b[0;32m    773\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[39m    Dict {group name -> group labels}.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39m    {Timestamp('2023-01-01 00:00:00'): 2, Timestamp('2023-02-01 00:00:00'): 4}\u001b[39;00m\n\u001b[0;32m    815\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 816\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49mgroups\n",
      "File \u001b[1;32mproperties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\spect\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:711\u001b[0m, in \u001b[0;36mBaseGrouper.groups\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    709\u001b[0m         to_groupby\u001b[39m.\u001b[39mappend(gv\u001b[39m.\u001b[39mgroupings[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mgrouping_vector)\n\u001b[0;32m    710\u001b[0m index \u001b[39m=\u001b[39m MultiIndex\u001b[39m.\u001b[39mfrom_arrays(to_groupby)\n\u001b[1;32m--> 711\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis\u001b[39m.\u001b[39;49mgroupby(index)\n",
      "File \u001b[1;32mc:\\Users\\spect\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6373\u001b[0m, in \u001b[0;36mIndex.groupby\u001b[1;34m(self, values)\u001b[0m\n\u001b[0;32m   6371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(values, ABCMultiIndex):\n\u001b[0;32m   6372\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 6373\u001b[0m values \u001b[39m=\u001b[39m Categorical(values)\n\u001b[0;32m   6374\u001b[0m result \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39m_reverse_indexer()\n\u001b[0;32m   6376\u001b[0m \u001b[39m# map to the label\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\spect\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\categorical.py:449\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, values, categories, ordered, dtype, fastpath, copy)\u001b[0m\n\u001b[0;32m    447\u001b[0m     values \u001b[39m=\u001b[39m sanitize_array(values, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    448\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 449\u001b[0m     codes, categories \u001b[39m=\u001b[39m factorize(values, sort\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    451\u001b[0m     codes, categories \u001b[39m=\u001b[39m factorize(values, sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\spect\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:802\u001b[0m, in \u001b[0;36mfactorize\u001b[1;34m(values, sort, use_na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    795\u001b[0m     codes, uniques \u001b[39m=\u001b[39m factorize_array(\n\u001b[0;32m    796\u001b[0m         values,\n\u001b[0;32m    797\u001b[0m         use_na_sentinel\u001b[39m=\u001b[39muse_na_sentinel,\n\u001b[0;32m    798\u001b[0m         size_hint\u001b[39m=\u001b[39msize_hint,\n\u001b[0;32m    799\u001b[0m     )\n\u001b[0;32m    801\u001b[0m \u001b[39mif\u001b[39;00m sort \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 802\u001b[0m     uniques, codes \u001b[39m=\u001b[39m safe_sort(\n\u001b[0;32m    803\u001b[0m         uniques,\n\u001b[0;32m    804\u001b[0m         codes,\n\u001b[0;32m    805\u001b[0m         use_na_sentinel\u001b[39m=\u001b[39;49muse_na_sentinel,\n\u001b[0;32m    806\u001b[0m         assume_unique\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    807\u001b[0m         verify\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    808\u001b[0m     )\n\u001b[0;32m    810\u001b[0m uniques \u001b[39m=\u001b[39m _reconstruct_data(uniques, original\u001b[39m.\u001b[39mdtype, original)\n\u001b[0;32m    812\u001b[0m \u001b[39mreturn\u001b[39;00m codes, uniques\n",
      "File \u001b[1;32mc:\\Users\\spect\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1637\u001b[0m, in \u001b[0;36msafe_sort\u001b[1;34m(values, codes, use_na_sentinel, assume_unique, verify)\u001b[0m\n\u001b[0;32m   1633\u001b[0m     sorter \u001b[39m=\u001b[39m ensure_platform_int(t\u001b[39m.\u001b[39mlookup(ordered))\n\u001b[0;32m   1635\u001b[0m \u001b[39mif\u001b[39;00m use_na_sentinel:\n\u001b[0;32m   1636\u001b[0m     \u001b[39m# take_nd is faster, but only works for na_sentinels of -1\u001b[39;00m\n\u001b[1;32m-> 1637\u001b[0m     order2 \u001b[39m=\u001b[39m sorter\u001b[39m.\u001b[39margsort()\n\u001b[0;32m   1638\u001b[0m     new_codes \u001b[39m=\u001b[39m take_nd(order2, codes, fill_value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   1639\u001b[0m     \u001b[39mif\u001b[39;00m verify:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop over folds\n",
    "for j in range(n_datasets):\n",
    "\n",
    "    # Get a pair of training and test sets\n",
    "    train = train_datasets[j]\n",
    "    test = test_datasets[j]\n",
    "\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    # Identify the distinct store/dept pairs shared by the training and test set.\n",
    "    # Will only process these.\n",
    "\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "    \n",
    "    # Join the distinct store/dept pairs to the training set.\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "    \n",
    "    # Now join the distinct store/dept pairs to the test set.\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    \n",
    "    #print('pre-smooth')\n",
    "    #print(train_split)\n",
    "\n",
    "    # Smooth weekly sales in the training dataset\n",
    "    train_smooth = smooth_weekly_sales(train_split, j, n_components)\n",
    "    \n",
    "    # Convert training sales dates to years + weeks\n",
    "    train_split = dates_to_years_and_weeks(train_smooth)\n",
    "    train_split = train_split.sort_values(by=['Store', 'Dept', 'Date'], ascending=[True, True, True])\n",
    "\n",
    "    #print('post-smooth')\n",
    "    #print(train_split)\n",
    "    \n",
    "    #print(\"train_smooth columns:\", train_smooth.columns)\n",
    "    \n",
    "    # Get design matrices for training y and X.\n",
    "    # y is just the target variable, Weekly_Sales.\n",
    "    # X has pivoted weeks, where individual weeks are separate 0/1 columns.\n",
    "    y, X = patsy.dmatrices('Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr  + Wk', \n",
    "                        data = train_split, \n",
    "                        return_type='dataframe')\n",
    "\n",
    "    # Group by store and department to help build separate model for store/dept combo.\n",
    "    # Create dict where key = (Store, Dept) and value = dataframe of Store/Date/Weekly_Sales.\n",
    "    #train_split = dict(tuple(train_split.groupby([\"Dept\"])))\n",
    "    train_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    # Convert test sales dates to years + weeks\n",
    "    test_split = dates_to_years_and_weeks(test_split)\n",
    "\n",
    "    # Get design matrices for text y and X.\n",
    "    # y is the Year, and the design matrix is \"Store + Dept + Yr + Wk\".\n",
    "    # Note that test sets don't have the Weekly_Sales target variable.\n",
    "    y, X = patsy.dmatrices('Yr ~ Store + Dept + Yr  + Wk', \n",
    "                        data = test_split, \n",
    "                        return_type='dataframe')\n",
    "    \n",
    "    # Re-add Date column to the design matrix X\n",
    "    X['Date'] = test_split['Date']\n",
    "    # Get dictionary where keys are (Store, Dept) tuples, and values are the\n",
    "    # \\\"Yr  + Wk + Date\\\" design matrices corresponding to each key.\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "    \n",
    "    # Get the training departments\n",
    "    keys = list(train_split)\n",
    "\n",
    "    # Loop over (store, dept) tuples\n",
    "    for key in keys:\n",
    "        \n",
    "        #print(\"Key:\", key)\n",
    "\n",
    "        # Get training and test design matrices corresponding to (store, dept)\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "    \n",
    "        # Target variable for (store, dept)\n",
    "        Y = X_train['Weekly_Sales']\n",
    "        # Drop ID and target to get just a table of predictors\n",
    "        X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)\n",
    "        \n",
    "        # Identify columns that are all zero in training predictors, and drop them\n",
    "        # from both training and test X.\n",
    "        # This should drop weeks that are not represented in the training data.\n",
    "        # How does this affect test X? Are there cases where all test weeks would be dropped?\n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        \n",
    "        # Identify X training columns that are highly collinear with the columns to the left.\n",
    "        # Note that this doesn't check the Intercept column.\n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "\n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-10:\n",
    "                    cols_to_drop.append(col_name)\n",
    "                \n",
    "        # Drop those collinear columns from both training and test X.\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        #add quadratic Yrs\n",
    "        #print(X_train.columns)\n",
    "        try:\n",
    "            X_train[\"Yr^2\"] = X_train[\"Yr\"]*X_train[\"Yr\"]\n",
    "            X_test[\"Yr^2\"] = X_test[\"Yr\"]*X_test[\"Yr\"]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Fit a regular ordinary least squares model on training Weekly_Sales.\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "        \n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "        \n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "        \n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "\n",
    "    #Replace negative predictions with 0\n",
    "    test_pred.loc[test_pred['Weekly_Pred'] < 0, 'Weekly_Pred'] = 0\n",
    "\n",
    "    #Merge with original test set for this fold to capture unpredicted values (not in training) and set to 0\n",
    "    test_pred = pd.merge(test_datasets[j], test_pred, how=\"left\", on=[\"Store\", \"Dept\", \"Date\"]).fillna(0)\n",
    "    \n",
    "    # Save the output to CSV\n",
    "    file_path = f'Data/fold_{j+1}/mypred.csv'\n",
    "    print(f'fold_{j+1} processed')\n",
    "    test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t1943.432\n",
      "\t1360.761\n",
      "\t1381.853\n",
      "\t1526.242\n",
      "\t2316.271\n",
      "\t1630.953\n",
      "\t1611.095\n",
      "\t1353.463\n",
      "\t1335.697\n",
      "\t1332.011\n",
      "1579.178\n"
     ]
    }
   ],
   "source": [
    "wae = myeval()\n",
    "for value in wae:\n",
    "    print(f\"\\t{value:.3f}\")\n",
    "print(f\"{sum(wae) / len(wae):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yr^2 not removing negatives in trainin\t\n",
    "\n",
    "1945.911\n",
    "1364.181\n",
    "1384.114\n",
    "1528.761\n",
    "2320.364\n",
    "1638.587\n",
    "1614.787\n",
    "1355.643\n",
    "1337.016\n",
    "1334.433\n",
    "\n",
    "1582.380\n",
    "\n",
    "yr^2 removing negatives in trainin\n",
    "\n",
    "1946.054\n",
    "1363.932\n",
    "1383.931\n",
    "1528.226\n",
    "2320.131\n",
    "1638.557\n",
    "1614.712\n",
    "1355.654\n",
    "1337.036\n",
    "1334.380\n",
    "\n",
    "1582.261\n",
    "\n",
    "yr^3 removing negatives in trainin\n",
    "\n",
    "1946.051\n",
    "1363.931\n",
    "1383.931\n",
    "1528.226\n",
    "2320.131\n",
    "1638.970\n",
    "1614.709\n",
    "1355.654\n",
    "1337.044\n",
    "1334.387\n",
    "\n",
    "1582.304\n",
    "\n",
    "yr^2 removing negatives in training and predictions\n",
    "\n",
    "1943.624\n",
    "1360.557\n",
    "1381.705\n",
    "1525.914\n",
    "2316.151\n",
    "1630.949\n",
    "1611.057\n",
    "1353.492\n",
    "1335.722\n",
    "1331.992\n",
    "\n",
    "1579.116\n",
    "\n",
    "yr^2 only removing negatives in predictions\n",
    "\n",
    "1943.432\n",
    "1360.761\n",
    "1381.853\n",
    "1526.242\n",
    "2316.271\n",
    "1630.953\n",
    "1611.095\n",
    "1353.463\n",
    "1335.697\n",
    "1332.011\n",
    "\n",
    "1579.178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
