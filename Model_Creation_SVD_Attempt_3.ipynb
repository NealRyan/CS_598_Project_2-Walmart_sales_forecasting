{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "\n",
    "# Import Pandas, ignoring SettingWithCopyWarning notifications\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "\n",
    "# Set seed\n",
    "SEED = 4031\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data file locations and names\n",
    "\n",
    "project_root_dir = \"Data\"\n",
    "project_subdir_prefix = \"fold_\"\n",
    "train_data_filename = \"train.csv\"\n",
    "test_data_filename = \"test.csv\"\n",
    "\n",
    "\n",
    "# The number of train/test data folders and the target RMSE for each\n",
    "# train/test split in each folder\n",
    "\n",
    "n_datasets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of data subfolders, each with a separate training and test set.\n",
    "\n",
    "os_walk = os.walk(project_root_dir)\n",
    "data_subdir_list = [subdirs for root, subdirs, files in os_walk][0]\n",
    "n_subdirs = len(data_subdir_list)\n",
    "\n",
    "assert(n_subdirs == n_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists for training and test datasets\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "\n",
    "# Loop over subfolders and read in training/test datasets and test weekly sales.\n",
    "# Use a loop instead of using os.walk directly to avoid \"fold10\" immediately following \"fold1\".\n",
    "\n",
    "for subdir_num in np.arange(n_subdirs) + 1:\n",
    "    subdir_num_str = str(subdir_num)\n",
    "    train_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   train_data_filename)))\n",
    "    test_datasets.append(pd.read_csv(os.path.join(project_root_dir,\n",
    "                                                   project_subdir_prefix + subdir_num_str,\n",
    "                                                   test_data_filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a WMAE function for scoring\n",
    "\n",
    "def wmae():\n",
    "    file_path = 'Data/test_with_label.csv'\n",
    "    test = pd.read_csv(file_path)\n",
    "    num_folds = 10\n",
    "    wae = []\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        file_path = f'Data/fold_{i+1}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    return wae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit original OLS: group stores by department and add SVD/PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps\n",
    "1. Group training data by department\n",
    "2. Pivot each department's data so that stores are rows and dates are columns, with values = weekly sales\n",
    "3. Fill in missing stores and dates, setting their sales to zero\n",
    "4. Center store values\n",
    "5. Perform SVD\n",
    "6. Re-add store means\n",
    "7. Use the SVD output as y_train for x_train = \\[Year, Week, Store\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components to return from SVD. This is from the example in Campuswire post #364:\n",
    "# https://campuswire.com/c/G06C55090/feed/364\n",
    "n_components = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_weekly_sales(train_data, n_components=n_components):\n",
    "    \"\"\"\n",
    "    Given a fold's training dataset of weekly sales by store and department,\n",
    "    use SVD to smooth each department's weekly sales across stores.\n",
    "    Return the dataset with smoothed sales.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store each department's processed data\n",
    "    dept_store_list = []\n",
    "    smoothed_sales_list = []\n",
    "    \n",
    "    t_split = dict(tuple(train_data.groupby([\"Dept\"])))\n",
    "    \n",
    "    depts = list(t_split)\n",
    "    \n",
    "    for dept in depts:\n",
    "        \n",
    "        # Get needed columns for department\n",
    "        t_data = t_split[dept]\n",
    "        #print(\"t_data columns:\", t_data.columns)\n",
    "        \n",
    "        dept_store_list.append(t_data[[\"Dept\", \"Store\"]].reset_index())\n",
    "        \n",
    "        # Pivot each store/dept combo's sales by date.\n",
    "        # Rows and stores are depts, columns are dates, values are sales figures.\n",
    "        t_pivot = t_data.pivot(index=[\"Dept\", \"Store\"], columns=\"Date\", values=\"Weekly_Sales\").reset_index().fillna(0)\n",
    "        # Save list of Date values for dept\n",
    "        t_dates = t_pivot.columns[2:]\n",
    "        #print(\"t_dates:\", t_dates)\n",
    "      \n",
    "        # Extract just the dates and sales for further processing\n",
    "        t_sales = t_pivot.to_numpy()[:, 2:]\n",
    "        t_dept_store = t_pivot.to_numpy()[:, :2]\n",
    "        \n",
    "        # Get store means\n",
    "        t_store_means = np.mean(t_sales, axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Center sales data\n",
    "        t_centered_sales = t_sales - t_store_means\n",
    "    \n",
    "        # Perform SVD on centered sales data\n",
    "        t_U, t_S, t_V = np.linalg.svd(t_centered_sales)\n",
    "        \n",
    "        # If there are at least n_components components, perform smoothing\n",
    "        if (t_S.shape[0] >= n_components):\n",
    "    \n",
    "            # Reduce the number of components\n",
    "            t_U_reduced = t_U[:, :n_components]\n",
    "            t_D_reduced = np.diag(t_S[:n_components])\n",
    "            t_Vt_reduced = t_V[:, :n_components].T\n",
    "        \n",
    "            #print(\"U, D, Vt dimensions:\", t_U_reduced.shape, t_D_reduced.shape, t_Vt_reduced.shape)\n",
    "    \n",
    "            # Regenerate smoothed sales\n",
    "            t_sales_smooth = (t_U_reduced @ t_D_reduced @ t_Vt_reduced) + t_store_means\n",
    "\n",
    "        # Otherwise just use original sales\n",
    "        else:\n",
    "            t_sales_smooth = t_sales\n",
    "        \n",
    "        #print(\"t_sales_smooth:\", t_sales_smooth)\n",
    "        \n",
    "        # Join Store and Dept back to smoothed sales figures\n",
    "        #t_dept_store_sales_smooth = np.concatenate((t_dept_store, t_sales_smooth), axis=1)\n",
    "        #t_columns = [\"Dept\", \"Store\"].extend(t_dates)\n",
    "        \n",
    "        #print(\"t_columns:\", t_columns)\n",
    "    \n",
    "        # Convert smoothed sales from array to dataframe\n",
    "        t_sales_smooth_df = pd.DataFrame(t_sales_smooth, columns=t_dates)\n",
    "        \n",
    "        # Unpivot the smoothed sales\n",
    "        t_sales_unpivot = t_sales_smooth_df.melt(var_name=\"Date\", value_name=\"Weekly_Sales\").reset_index()\n",
    "\n",
    "        # Save dept's smoothed sales\n",
    "        smoothed_sales_list.append(t_sales_unpivot)\n",
    "    \n",
    "    # Stack the accumulated lists of dataframes\n",
    "    t_dept_store = pd.concat(dept_store_list, axis=0, ignore_index=True)\n",
    "    #print(\"t_dept_store columns:\", t_dept_store.columns)\n",
    "    t_sales = pd.concat(smoothed_sales_list, axis=0, ignore_index=True)\n",
    "    #print(\"t_sales columns:\", t_sales.columns)\n",
    "    # Join Store/Dept back to sales\n",
    "    t_dept_store_sales = pd.concat([t_dept_store, t_sales], axis=1)\n",
    "    #print(\"t_dept_store_sales columns:\", t_dept_store_sales.columns)\n",
    "    \n",
    "    # Re-join store/dept dataframe with smoothed-sales dataframe\n",
    "    return t_dept_store_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_to_years_and_weeks(data):\n",
    "    \"\"\"\n",
    "    Convert sales dates in data to numeric years and categorical weeks.\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = pd.to_datetime(data[\"Date\"])\n",
    "    data[\"Wk\"] = tmp.dt.isocalendar().week\n",
    "    data[\"Yr\"] = tmp.dt.year\n",
    "    data[\"Wk\"] = pd.Categorical(data[\"Wk\"], categories=[i for i in range(1, 53)])  # 52 weeks\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_1 processed\n",
      "fold_2 processed\n",
      "fold_3 processed\n",
      "fold_4 processed\n",
      "fold_5 processed\n",
      "fold_6 processed\n",
      "fold_7 processed\n",
      "fold_8 processed\n",
      "fold_9 processed\n",
      "fold_10 processed\n"
     ]
    }
   ],
   "source": [
    "# Loop over folds\n",
    "for j in range(n_datasets):\n",
    "\n",
    "    # Get a pair of training and test sets\n",
    "    train = train_datasets[j]\n",
    "    test = test_datasets[j]\n",
    "\n",
    "    test_pred = pd.DataFrame()\n",
    "\n",
    "    # Identify the distinct store/dept pairs shared by the training and test set.\n",
    "    # Will only process these.\n",
    "\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "    \n",
    "    # Join the distinct store/dept pairs to the training set.\n",
    "    # Why left join? When would training data not be available?\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "    #print(\"train_split columns:\", train_split.columns)\n",
    "    \n",
    "    # Now join the distinct store/dept pairs to the test set.\n",
    "    # Same question: why left join? When would training data not be available?\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    \n",
    "    # Smooth weekly sales in the training dataset\n",
    "    train_smooth = smooth_weekly_sales(train_split)\n",
    "    \n",
    "    # Convert training sales dates to years + weeks\n",
    "    train_split = dates_to_years_and_weeks(train_smooth)\n",
    "    \n",
    "    #print(\"train_smooth columns:\", train_smooth.columns)\n",
    "    \n",
    "    # Get design matrices for training y and X.\n",
    "    # y is just the target variable, Weekly_Sales.\n",
    "    # X has pivoted weeks, where individual weeks are separate 0/1 columns.\n",
    "    y, X = patsy.dmatrices('Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr  + Wk', \n",
    "                        data = train_split, \n",
    "                        return_type='dataframe')\n",
    "\n",
    "    # Group by store and department to help build separate model for store/dept combo.\n",
    "    # Create dict where key = (Store, Dept) and value = dataframe of Store/Date/Weekly_Sales.\n",
    "    #train_split = dict(tuple(train_split.groupby([\"Dept\"])))\n",
    "    train_split = dict(tuple(X.groupby([\"Store\", \"Dept\"])))\n",
    "\n",
    "    # Convert test sales dates to years + weeks\n",
    "    test_split = dates_to_years_and_weeks(test_split)\n",
    "\n",
    "    # Get design matrices for text y and X.\n",
    "    # y is the Year, and the design matrix is \"Store + Dept + Yr + Wk\".\n",
    "    # Note that test sets don't have the Weekly_Sales target variable.\n",
    "    y, X = patsy.dmatrices('Yr ~ Store + Dept + Yr  + Wk', \n",
    "                        data = test_split, \n",
    "                        return_type='dataframe')\n",
    "    \n",
    "    # Re-add Date column to the design matrix X\n",
    "    X['Date'] = test_split['Date']\n",
    "    # Get dictionary where keys are (Store, Dept) tuples, and values are the\n",
    "    # \\\"Yr  + Wk + Date\\\" design matrices corresponding to each key.\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "    \n",
    "    # Get the training departments\n",
    "    keys = list(train_split)\n",
    "\n",
    "    # Loop over (store, dept) tuples\n",
    "    for key in keys:\n",
    "        \n",
    "        #print(\"Key:\", key)\n",
    "\n",
    "        # Get training and test design matrices corresponding to (store, dept)\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "    \n",
    "        # Target variable for (store, dept)\n",
    "        Y = X_train['Weekly_Sales']\n",
    "        # Drop ID and target to get just a table of predictors\n",
    "        X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)\n",
    "        \n",
    "        # Identify columns that are all zero in training predictors, and drop them\n",
    "        # from both training and test X.\n",
    "        # This should drop weeks that are not represented in the training data.\n",
    "        # How does this affect test X? Are there cases where all test weeks would be dropped?\n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        \n",
    "        # Identify X training columns that are highly collinear with the columns to the left.\n",
    "        # Note that this doesn't check the Intercept column.\n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "\n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-10:\n",
    "                    cols_to_drop.append(col_name)\n",
    "                \n",
    "        # Drop those collinear columns from both training and test X.\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "        #print(X_train)\n",
    "        # Fit a regular ordinary least squares model on training Weekly_Sales.\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "        \n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "        \n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "        \n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "    # Save the output to CSV\n",
    "    file_path = f'Data/fold_{j+1}/mypred.csv'\n",
    "    print(f'fold_{j+1} processed')\n",
    "    test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t14855.138\n",
      "\t14867.111\n",
      "\t15083.058\n",
      "\t14972.169\n",
      "\t17742.977\n",
      "\t15899.242\n",
      "\t15319.759\n",
      "\t15466.072\n",
      "\t15278.362\n",
      "\t14916.459\n",
      "15440.035\n"
     ]
    }
   ],
   "source": [
    "wae = wmae()\n",
    "for value in wae:\n",
    "    print(f\"\\t{value:.3f}\")\n",
    "print(f\"{sum(wae) / len(wae):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
